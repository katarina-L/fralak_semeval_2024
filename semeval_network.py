# -*- coding: utf-8 -*-
"""hierarchical_f1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xd_l9DKsFIwtYQcro_64Jil4rTB5X1Fj
"""

import random as rd
import re
from transformers import AutoTokenizer, AutoModelForTokenClassification
from sentence_transformers import SentenceTransformer
from transformers import pipeline
from langdetect import detect

rd.seed = 152

tokenizer = AutoTokenizer.from_pretrained("Babelscape/wikineural-multilingual-ner")
ner_model = AutoModelForTokenClassification.from_pretrained("Babelscape/wikineural-multilingual-ner")

nlp = pipeline("ner", model=ner_model, tokenizer=tokenizer, grouped_entities=True)

embed_model = SentenceTransformer('sentence-transformers/stsb-xlm-r-multilingual')

class Node:
  def __init__(self, name, is_label = False):
    self.name = name
    self.parents = []
    self.daughters = []
    self.is_label = is_label
    self.color = self.get_color(name)

  def get_color(self, name):
    grey = ['persuasion']
    red = ['ethos', 'pathos', 'logos']
    blue = ['hominem', 'justification', 'reasoning']
    green = ['distraction', 'simplification']
    if name in grey:
      return('grey')
    elif name in red:
      return('red')
    elif name in blue:
      return('blue')
    elif name in green:
      return('green')
    else:
      return('white')

class Label_Lang:
  def __init__(self, name):
    self.name = name
    self.words = ['SOS', 'END', 'persuasion', 'namely', 'black', 'logos', 'reasoning', 'simplification', 'loaded', 'generalities',
        'pathos', 'and', 'ethos', 'none', 'cliché', 'whataboutism', 'hominem', 'distraction', 'slogans', 'justification',
        'smears', 'oversimplification', 'names', 'authority', 'exaggeration', 'repetition', 'flag', 'fear', 'hitler',
        'doubt', 'scarecrow', 'vagueness', 'bandwagon', 'herring']
    self.voc_size = len(self.words)
    self.index2word = {}
    self.word2index = {}

  def add_label(self, labels):
    for l in labels:
      # l = 'SOS ' + l
      tokens = l.split(' ')
      for t in tokens:
        if not t in self.words:
          self.words.append(t)

  def create_lookups(self):
    self.voc_size = len(self.words)
    for i, w in enumerate(self.words):
      self.index2word[i] = w
      self.word2index[w] = i

  def translate_words(self, text):
    #input: a string
    #output: a list of indices
    tokens = text.split(' ')
    indices = []
    for t in tokens:
      if t in self.word2index:
        i = self.word2index[t]
        indices.append(i)
      else:
        print(t)
    return(indices)

  def translate_indices(self, indices):
    #input: a list of indices
    #output: a string of words
    text = []
    for i in indices:
      word = self.index2word[i]
      text.append(word)
    text = ' '.join(text)
    return(text)

class Semeval_Network:
  def __init__(self):
    self.lookup_dict = {'ad hominem' : 'hominem', #for the 1 word names
                   'name calling' : 'names',
                   'reductio ad hitlerum' : 'hitler',
                   'appeal to authority' : 'authority',
                   'glittering generalities' : 'generalities',
                   'appeal to emotion (visual)' : 'emotion',
                   'loaded language' : 'loaded',
                   'flag waving' : 'flag',
                   'appeal to fear' : 'fear',
                    'intentional vagueness' : 'vagueness',
                    'straw man' : 'scarecrow',
                    'red herring' : 'herring',
                    'presenting irrelevant data (red herring)' : 'herring',
                    'causal oversimplification' : 'oversimplification',
                    'black & white fallacy' : 'black',
                    'thought terminating cliché' : 'cliché',
                    'exaggeration/minimisation' : 'exaggeration'}

    self.labels_order = ['smears', 'loaded', 'names', 'authority', 'black', 'slogans',
                    'flag', 'cliché', 'generalities', 'exaggeration', 'doubt', 'fear',
                    'repetition', 'whataboutism', 'oversimplification', 'bandwagon',
                    'hitler', 'scarecrow', 'herring', 'vagueness']  # labels ordered by frequency in the training data

    self.node_names = self.get_node_names()
    self.nodes = self.initialize()
    self.leaves = []
    self.label_lang = Label_Lang('label_lang')
    # self.leaves is a list of dictionaries:
    # leave = {'text' : str(meme text), 'labels' : [str, str, str]}
    # after restructuring the leaves the key 'restructured_labels' is added: a list of strings
    # after filling the leaves with the add_leaves method we can access the parent
    # node of each leave by:
    """
    example_leaf = self.leaves[1]
    parent_nodes = example_leaf['labels']
    for node_name in parent_nodes:
      node_object = self.nodes[node_name]
    """

  def initialize(self):
    nodes = {x : Node(x) for x in self.node_names}
    # so we have a dictionary with the node names (keys) and Node objects (values)
    # now we're going to hard code all connections
    # amazing use of my wednesday afternoon <3

    # mark all labels as labels
    l = ['names', 'doubt', 'smears', 'hitler', 'bandwagon', 'authority',
         'generalities', 'emotion', 'exaggeration', 'loaded', 'flag',
         'fear', 'transfer', 'slogans', 'repetition', 'vagueness',
         'scarecrow', 'herring', 'whataboutism', 'oversimplification',
         'black', 'cliché']
    for lab in l:
      node = nodes[lab]
      node.is_label = True
      nodes[lab] = node

    #persuasion
    s = 'persuasion'
    d = ['ethos', 'logos', 'pathos']
    nodes = self.add_parent_daughter_nodes(nodes, s, d)

    #ethos
    s = 'ethos'
    d = ['hominem', 'transfer', 'generalities', 'bandwagon', 'authority']
    nodes = self.add_parent_daughter_nodes(nodes, s, d)

    #pathos
    s = 'pathos'
    d = ['emotion', 'exaggeration', 'loaded', 'flag', 'fear', 'transfer']
    nodes = self.add_parent_daughter_nodes(nodes, s, d)

    #logos
    s = 'logos'
    d = ['justification', 'reasoning', 'repetition', 'vagueness']
    nodes = self.add_parent_daughter_nodes(nodes, s, d)

    #ad hominem
    s = 'hominem'
    d = ['names', 'doubt', 'smears', 'hitler', 'whataboutism']
    nodes = self.add_parent_daughter_nodes(nodes, s, d)

    #justification
    s = 'justification'
    d = ['bandwagon', 'authority', 'slogans', 'flag', 'fear']
    nodes = self.add_parent_daughter_nodes(nodes, s, d)

    #reasoning
    s = 'reasoning'
    d = ['distraction', 'simplification']
    nodes = self.add_parent_daughter_nodes(nodes, s, d)

    #distraction
    s = 'distraction'
    d = ['scarecrow', 'herring', 'whataboutism']
    nodes = self.add_parent_daughter_nodes(nodes, s, d)

    #simplification
    s = 'simplification'
    d = ['oversimplification', 'black', 'cliché']
    nodes = self.add_parent_daughter_nodes(nodes, s, d)

    return(nodes)

  def add_parent_daughter_nodes(self, nodes, s, d):
    # s = string with the name of the parent node
    # d = list of strings of daughter nodes
    # nodes = dict with all nodes
    for n in d:
      node = nodes[n] # get the daughter node
      node.parents.append(s) # append the parent
      nodes[n] = node # replace the value with updated daughter node
    p = nodes[s] # get the parent node
    p.daughters += d # add all daughter names to the daughter list
    p.daughters = list(set(p.daughters)) # shouldn't be necessary but ok
    nodes[s] = p
    return(nodes)

  def get_node_names(self):
    node_names = []
    file_name = 'node_names.txt'
    try:
      with open(file_name, 'r') as file:
        for line in file:
          name = line.strip().lower()
          if name in self.lookup_dict:
            name = self.lookup_dict[name]
          node_names.append(name)
    except:
      print(f'File {file_name} not found!')
      return(None)
    return(node_names)

  def add_leaves(self, leaves):
    # leaves is a list of tuples (text, labels, id)
    # this updates the network and returns nothing
    for instance in leaves:
      #first, we normalize the label names
      text = instance[0]
      labels = instance[1]
      id = instance[2]
      new_labels = []
      for l in labels:
        l = l.strip().lower()
        l = re.sub('black-and-white', 'black & white', l)
        l = re.sub('/.*', '', l)
        #l = re.sub('/labeling', '', l)
        l = re.sub(' \(virtue\)', '', l)
        l = re.sub('-', ' ', l)
        if re.search('misrep', l) is not None:
          l = 'straw man'
        if re.search(' ', l) is not None:
          if l in self.lookup_dict:
            l = self.lookup_dict[l]
          elif re.search('vagueness', l) is not None:
            l = 'vagueness'
          else:
            print(l)
        new_labels.append(l)
      if not len(new_labels) == 0:
        new_labels.sort(key=lambda x: self.labels_order.index(x))
      instance = {'text' : text, 'labels' : new_labels, 'id' : id}
      self.leaves.append(instance)

  def restructure_all_leaves(self):
    #updates the 'self.leaves['restructured_leaves']' variable. This is a list of leaves
    #with their labels. This adds a key 'restructured_labels' that returns a
    #list of strings (sequences) which are all model outputs

    #okay now we loop over all leaves in order to restructure them
    for leaf in self.leaves:
      if len(leaf['labels']) == 0:
        restructured_labels = ['SOS none END']
        leaf['restructured_labels'] = restructured_labels
        self.label_lang.add_label(restructured_labels)
        continue
      else:
        #sort the labels by frequency using the frequency list
        paths = {}
        restructured_labels = []
        for label in leaf['labels']:
          label_parents = self.nodes[label].parents
          label_paths = [] #a list containing, for each parent, a dictionary with the colors (keys) and the nodes
          for p in label_parents:
            path = {}
            while len(self.nodes[p].parents) > 0: #while the current node has parents
              path[self.nodes[p].color] = str(p)
              p = str(self.nodes[p].parents[0])
            label_paths.append(path)
          paths[label] = label_paths

        red = "SOS persuasion"
        blue = "SOS persuasion"
        green = "SOS persuasion"
        grey = "SOS persuasion"

        for j, label in enumerate(leaf['labels']):
          for i, d in enumerate(paths[label]):
            #each d is a dictionary. Now we will make the labels
            if not j > 0:
              x = d['red']
              red += f" namely {x}"
              blue += f" namely {x}"
              green += f" namely {x}"
            else:
              x = d['red']
              red += f" and {x}"
              blue += f" and {x}"
              green += f" and {x}"
            #blue
            if 'blue' in d:
              x = d['blue']
              blue += f' namely {x}'
              green += f' namely {x}'
            #green
            if 'green' in d:
              x = d['green']
              green += f' namely {x}'

            red += f' namely {label}'
            blue += f' namely {label}'
            green += f' namely {label}'
            grey += f' namely {label}'

          if j == len(leaf['labels']) - 1:
            red += ' END'
            blue += ' END'
            green += ' END'
            grey += ' END'
        #print(f'{grey}\n{red}\n{blue}\n{green}\n') #a print to check if everything went well
        if green == blue:
          restructured_labels = [grey, red, blue]
        else:
          restructured_labels = [grey, red, blue, green]
        restructured_labels = [green] # COMMENT THIS LINE FOR THE ORIGINAL SUBMISSION
        leaf['restructured_labels'] = restructured_labels
        self.label_lang.add_label(restructured_labels)

    self.label_lang.create_lookups()

  def spelling(self, t):
    t = t.strip()
    t = re.sub(r'\\n', '\. ', t)
    t = re.sub(r'(.)\1{3,}', '\1\1', t) #characters that repeat too much
    t = re.sub(r'[AaHhJjХхАа]*[HhJjХх]?[AaАа]+[HhJjХх]+[AaАа]+[AaHhJjХхАа]*', 'haha', t) #haha-norm
    #t = re.sub('\'', '', t)
    t = re.sub('\W', ' ', t)
    t = re.sub('\s{2,}', ' ', t)
    t = re.sub('^ ', '', t)
    return(t)

  def preprocess_leaves(self):
    #this is a function to preprocess the leaves
    #it updates the self.leaves list by adding to all leaf dicts a 'preprocessed_text' key
    ##TODO make a people dict with the most occurring people and their names
    people_dict = {'Trump' : ['donald', 'donald trump', 'trump', 'donald j trump'],
              'Biden' : ['joe', 'joe biden', 'biden', 'bidens'],
              'Putin' : ['putin', 'vladimir putin', 'vladimir', 'vlad', 'vova', 'putins', 'vlads'],
              'Obama' : ['obama', 'barack obama', 'barack', 'obama\'s', 'hussein'],
              'Clinton' : ['clintons', 'hillary', 'hillary clinton', 'bill', 'bill clinton'], #i count them as 1 person lol
              'Reagan' : ['ronald reagan', 'reagan', 'ronald'],
              'Sanders' : ['bernie sanders', 'bernie', 'sanders'],
              'Pelosi' : ['nancy', 'nancy pelosi', 'pelosi']
              }
    lookup_dict = {}
    for p in people_dict:
      for alias in people_dict[p]:
        lookup_dict[alias] = p
    people_list = ['Trump', 'Biden', 'Putin', 'Obama', 'Clinton', 'Sanders', 'Reagan', 'Jesus',
                  'Pelosi']

    for leaf in self.leaves:
      #some basic spelling normalization
      new_text = self.spelling(leaf['text'])
      #add the language to the leaf dictionary
      try:
        leaf['ling'] = detect(new_text)
      except:
        leaf['ling'] = 'en'

      #replace all entities with 'PERSON', apart from the 30 most popular ones
      #maybe consider leaving this out as it's very slow haha

      ner_results = nlp(new_text) #outputs a list of dictionaries
      for e in ner_results:
        if e['entity_group'] == 'PER' and e['score'] > 0.8:
          word = e['word']
          if word.lower() in lookup_dict:
            ref_name = lookup_dict[word.lower()]
            new_text = re.sub(word, ref_name, new_text)
          else:
            new_text = re.sub(word, 'Mark', new_text) #everyone is Mark now

      new_text = new_text.lower()
      leaf['preprocessed_input'] = new_text

  def add_sentence_embeddings(self):
    for leaf in self.leaves:
      text = leaf['preprocessed_input']
      text = re.split('[\.\\n]', text) #actually the way we split sentence is something to experiment with
      leaf["embed"] = embed_model.encode(text)

"""#Calculate metrics

So I calculate the F1 in the following way:

FOR EACH LABEL:

recall = (overlap selected nodes & correct nodes) / all correct nodes

precision = (overlap selected nodes & correct nodes) / all selected nodes

F1 = 2 * (precision * recall / (precision + recall))

final scores are just the average over all instances

NB for 'none' i took zero overlapping nodes with the graph, so if y = none and y hat = none i gave all metrics a score of 1, if y = none and ŷ = something (or the other way around) i gave all metrics a score of 0. Idk if that was a good idea tho
"""

def get_all_parent_nodes(y):
  #in: a list of node names
  #out: a set of parent nodes
  all_parents = y #list were we will collect all parent nodes + the current ones

  for n in y: #for node in predictions/truths
    to_explore_stack = [n] #make a list with nodes we need to explore
    while len(to_explore_stack) > 0:
      node = network.nodes[n]
      if len(node.parents) > 0: #if this node has parents
        all_parents += node.parents
        to_explore_stack += node.parents #add them to the stack
        to_explore_stack = list(set(to_explore_stack)) #remove doubles
      to_explore_stack.remove(n)
      if len(to_explore_stack) > 0:
        n = to_explore_stack[0]
  return(set(all_parents))

def get_metrics(preds, truth):
  # preds = a list of lists of predicted labels
  # truth = an index-corresponding list of lists with the ground truth labels
  tot_f1 = []
  tot_recall = []
  tot_prec = []
  for i, y in enumerate(truth):
    y_hat = preds[i]
    if len(y_hat) == 0 or len(y) == 0:
      if len(y) == len(y_hat):
        tot_f1.append(1)
        tot_recall.append(1)
        tot_prec.append(1)
      else:
        tot_f1.append(0)
        tot_recall.append(0)
        tot_prec.append(0)
    else:
      y_all = get_all_parent_nodes(y)
      y_hat_all = get_all_parent_nodes(y_hat)
      n_overlap = len(list(y_all & y_hat_all))
      recall = n_overlap / len(y_all)
      prec = n_overlap / len(y_hat_all)
      tot_recall.append(recall)
      tot_prec.append(prec)
      f1 = 2 * (prec * recall / (prec + recall))
      tot_f1.append(f1)
  av_f1 = sum(tot_f1) / len(tot_f1)
  av_recall = sum(tot_recall) / len(tot_recall)
  av_prec = sum(tot_prec) / len(tot_prec)
  return(av_f1, av_recall, av_prec)

#generate some toy data to test if this thing is working

def get_random_daughter_label(n):
  while not network.nodes[n].is_label:
    n_daughters = len(network.nodes[n].daughters)
    i = rd.randint(0, n_daughters-1) #select a random daughter
    n = network.nodes[n].daughters[i]
  return(n)

network = Semeval_Network()

def try_something(N_SAMPLES = 200, threshold = 0.6):
  nodes = network.nodes
  n = nodes['hominem']
  print(n.daughters)
  print(n.parents)
  preds = []
  truths = []
  label_names = [network.nodes[x].name for x in network.nodes if network.nodes[x].is_label]
  for i in range(N_SAMPLES):
    n_y_hat = rd.randint(0,3)
    y_hat = rd.sample(network.node_names, n_y_hat)
    if rd.random() > 0.4:
      y = []
      for l in y_hat:
        d = get_random_daughter_label(l)
        y.append(d)
    else:
      n_y = rd.randint(0,3)
      y = rd.sample(label_names, n_y)
    preds.append(y_hat)
    truths.append(y)
  print(len(preds), len(truths))
  metrics = get_metrics(preds, truths)